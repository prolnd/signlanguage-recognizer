{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand Landmark Extraction Using MediaPipe\n",
    "\n",
    "Extracts hand landmarks from images using **MediaPipe Hands** and saves the processed data as a CSV file. It performs the following steps:\n",
    "\n",
    "- Loads and processes images from a specified dataset directory.\n",
    "- Uses **MediaPipe Hands** to detect hand landmarks.\n",
    "- Extracts and normalizes (x, y) coordinates for each of the 21 landmarks.\n",
    "- Saves the processed data into a structured DataFrame.\n",
    "- Outputs the data as a CSV file for further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, \n",
    "                      max_num_hands=1,\n",
    "                      min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "def extract_landmarks(image_path):\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read image: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert image from BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process image with MediaPipe\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    # If no hand detected, return None\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return None\n",
    "    \n",
    "    # Get landmarks of the first hand\n",
    "    hand_landmarks = results.multi_hand_landmarks[0]\n",
    "    \n",
    "    # Extract x, y coordinates of each landmark (21 landmarks with x, y)\n",
    "    landmarks = []\n",
    "    h, w, _ = image.shape\n",
    "    for landmark in hand_landmarks.landmark:\n",
    "        # Normalize coordinates to be relative to image size\n",
    "        x, y = landmark.x, landmark.y\n",
    "        landmarks.extend([x, y])\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "def process_dataset(base_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for gesture_folder in os.listdir(base_dir):\n",
    "        gesture_path = os.path.join(base_dir, gesture_folder)\n",
    "\n",
    "        if os.path.isdir(gesture_path):\n",
    "\n",
    "            counter = 0\n",
    "            # Limiting to speed up testing and avoid long processing times while working on the project\n",
    "\n",
    "            max_images = 100  \n",
    "            # Process each image in the gesture folder\n",
    "            for image_file in os.listdir(gesture_path):\n",
    "                if counter >= max_images:\n",
    "                    break   \n",
    "\n",
    "                image_path = os.path.join(gesture_path, image_file)\n",
    "                landmarks = extract_landmarks(image_path)\n",
    "                \n",
    "                if landmarks is not None:\n",
    "                    data.append(landmarks)\n",
    "                    labels.append(gesture_folder)\n",
    "                    counter += 1\n",
    "            \n",
    "            print(f\"Processed images for gesture: {gesture_folder}\")\n",
    "        \n",
    "    # Create DataFrame\n",
    "    landmark_names = []\n",
    "    for i in range(21):\n",
    "        landmark_names.extend([f'x{i}', f'y{i}'])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=landmark_names)\n",
    "    df['gesture'] = labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "dataset = \"images/asl_alphabet_train\"\n",
    "landmarks_df = process_dataset(dataset)\n",
    "landmarks_df.to_csv(\"hand_landmarks_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert dataframe to trainable format\n",
    "\n",
    "The `convert_to_trainable_format` function processes a dataset containing hand gesture data and prepares it for machine learning training. It performs the following steps:\n",
    "\n",
    "- Handles missing values by dropping them if any exist.\n",
    "- Encodes categorical labels (gesture names) into numerical values.\n",
    "- Normalizes feature values using `StandardScaler`.\n",
    "- Splits the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_trainable_format(df):\n",
    "\n",
    "    if df.isnull().values.any():\n",
    "        print(\"Warning: Dataset contains missing values\")\n",
    "        df = df.dropna()\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df['gesture'])\n",
    "    \n",
    "    X = df.drop('gesture', axis=1).values\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape[0]}\")\n",
    "    print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "    print(f\"Number of gestures: {len(label_encoder.classes_)}\")\n",
    "    print(f\"Gestures: {label_encoder.classes_}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, label_encoder, scaler\n",
    "\n",
    "csv_path = \"hand_landmarks_dataset.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "X_train, X_test, y_train, y_test, label_encoder, scaler = convert_to_trainable_format(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test, label_encoder):\n",
    "\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'SVM': SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "        'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results[name] = accuracy\n",
    "        \n",
    "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "        \n",
    "        # Save model\n",
    "        joblib.dump(model, f\"gesture_recognition_{name.replace(' ', '_').lower()}.pkl\")\n",
    "    \n",
    "    # Find the best model\n",
    "    best_model_name = max(results, key=results.get)\n",
    "    print(f\"\\nBest model: {best_model_name} with accuracy {results[best_model_name]:.4f}\")\n",
    "    \n",
    "    return models[best_model_name]\n",
    "\n",
    "best_model = train_and_evaluate_models(X_train, X_test, y_train, y_test, label_encoder)\n",
    "\n",
    "# Save preprocessing tools for later use\n",
    "joblib.dump(scaler, \"gesture_recognition_scaler.pkl\")\n",
    "joblib.dump(label_encoder, \"gesture_recognition_label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742201245.178661  323478 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742201245.181581  358762 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3.4-arch1.1), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 19.1.7, DRM 3.59, 6.13.1-arch2-1)\n",
      "W0000 00:00:1742201245.202229  358748 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742201245.223174  358754 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load(\"gesture_recognition_neural_network.pkl\")  \n",
    "scaler = joblib.load(\"gesture_recognition_scaler.pkl\")\n",
    "label_encoder = joblib.load(\"gesture_recognition_label_encoder.pkl\")\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, \n",
    "                       max_num_hands=1,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.8)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def recognize_gesture_from_camera():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "        \n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which MediaPipe uses)\n",
    "        image = cv2.flip(image, 1)  # Mirror display\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the image and detect hands\n",
    "        results = hands.process(image_rgb)\n",
    "        \n",
    "        # Draw hand landmarks\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                landmarks = []\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    landmarks.extend([landmark.x, landmark.y])\n",
    "                \n",
    "                landmarks_array = np.array(landmarks).reshape(1, -1)\n",
    "                landmarks_scaled = scaler.transform(landmarks_array)\n",
    "                \n",
    "                prediction = model.predict(landmarks_scaled)[0]\n",
    "                gesture_name = label_encoder.inverse_transform([prediction])[0]\n",
    "                \n",
    "                # Display prediction\n",
    "                cv2.putText(image, f\"Gesture: {gesture_name}\", (10, 50), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the image\n",
    "        cv2.imshow('Hand Gesture Recognition', image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:  # ESC key to exit\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "recognize_gesture_from_camera()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
